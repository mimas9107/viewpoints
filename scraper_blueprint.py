#!/usr/bin/env python3
"""
tw.live åˆ†é¡ç™¼ç¾å™¨
éæ­·å…¨ç«™ä¸¦æ”¶é›†æ‰€æœ‰ç›£æ§é»åˆ†é¡é é¢
ç‰ˆæœ¬ï¼š2.0.1
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin


class TWLiveCategoryDiscoverer:
    def __init__(self):
        self.base_url = "https://tw.live"
        self.categories = {}
        self.endpoints = []  # å¯¦éš›æœ‰ç›£æ§é»çš„çµ‚é»é é¢
        self.visited = set()
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )

    def fetch_page(self, url):
        """æŠ“å–é é¢ä¸¦è™•ç†éŒ¯èª¤"""
        if url in self.visited:
            return None
        self.visited.add(url)

        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
            return None

    def extract_main_categories(self, html):
        """å¾ä¸»é æå–ä¸»è¦åˆ†é¡"""
        soup = BeautifulSoup(html, "html.parser")
        categories = {}

        # å¾ nav-scroller æå–åˆ†é¡é€£çµ
        nav = soup.find("div", class_="nav-scroller")
        if nav:
            links = nav.find_all("a", class_="nav-link")
            for link in links:
                href = link.get("href")
                if href and not href.startswith("#"):
                    text = link.get_text(strip=True)
                    if text and href != "/":
                        categories[text] = urljoin(self.base_url, href)

        return categories

    def extract_menu_categories(self, html):
        """å¾é é¢æå– cctv-menu ä¸­çš„åˆ†é¡é€£çµ"""
        soup = BeautifulSoup(html, "html.parser")
        categories = []

        menus = soup.find_all("div", class_="cctv-menu")
        for menu in menus:
            links = menu.find_all("a")
            for link in links:
                href = link.get("href")
                if href and not href.startswith("#"):
                    text = link.get_text(strip=True)
                    if text:
                        full_url = urljoin(self.base_url, href)
                        categories.append((text, full_url))

        return categories

    def extract_button_categories(self, html):
        """å¾é é¢æå–æŒ‰éˆ•é¡å‹çš„åˆ†é¡é€£çµï¼ˆç”¨æ–¼åœ‹é“ç­‰ï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        categories = []

        buttons = soup.find_all("a", class_="btn")
        for btn in buttons:
            if btn.get_text(strip=True) == "å³æ™‚å½±åƒ":
                href = btn.get("href")
                if href:
                    # å¾æŒ‰éˆ•é™„è¿‘çš„æ–‡å­—æå–åç¨±
                    container = btn.find_parent("div", class_="col-md-4")
                    if container:
                        h2 = container.find("h2")
                        if h2:
                            text = h2.get_text(strip=True)
                            full_url = urljoin(self.base_url, href)
                            categories.append((text, full_url))

        return categories

    def is_endpoint_page(self, html):
        """åˆ¤æ–·é é¢æ˜¯å¦ç‚ºçµ‚é»é é¢ï¼ˆæœ‰ cctv-stackï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        stacks = soup.find_all("div", class_="cctv-stack")
        return len(stacks) > 0

    def discover_categories(self):
        """ç™¼ç¾æ‰€æœ‰åˆ†é¡"""
        print("ğŸ  æŠ“å–ä¸»é ...")
        html = self.fetch_page(self.base_url)
        if not html:
            return

        # æå–ä¸»è¦åˆ†é¡
        main_categories = self.extract_main_categories(html)
        print(f"ğŸ“‚ æ‰¾åˆ° {len(main_categories)} å€‹ä¸»è¦åˆ†é¡")

        all_categories = {}

        for name, url in main_categories.items():
            print(f"\nğŸ—‚ï¸  è™•ç†åˆ†é¡: {name} ({url})")
            html = self.fetch_page(url)
            if not html:
                continue

            if self.is_endpoint_page(html):
                # ç›´æ¥æ˜¯çµ‚é»é é¢
                print(f"  âœ… çµ‚é»é é¢: {url}")
                self.endpoints.append({"name": name, "url": url, "type": "direct"})
            else:
                # ç¸½è¦½é é¢ï¼Œæå–å­åˆ†é¡
                sub_categories = self.extract_menu_categories(html)
                if not sub_categories:
                    sub_categories = self.extract_button_categories(html)
                print(f"  ğŸ“‹ æ‰¾åˆ° {len(sub_categories)} å€‹å­åˆ†é¡")
                all_categories[name] = sub_categories

                # æª¢æŸ¥å­åˆ†é¡æ˜¯å¦ç‚ºçµ‚é»
                for sub_name, sub_url in sub_categories:
                    html = self.fetch_page(sub_url)
                    if html:
                        if self.is_endpoint_page(html):
                            print(f"    âœ… çµ‚é»é é¢: {sub_name} ({sub_url})")
                            self.endpoints.append(
                                {
                                    "name": f"{name} - {sub_name}",
                                    "url": sub_url,
                                    "type": "sub",
                                    "parent": name,
                                }
                            )
                        else:
                            # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æœ‰å­å­åˆ†é¡
                            sub_sub = self.extract_menu_categories(html)
                            if sub_sub:
                                print(f"    ğŸ“‹ {sub_name} æœ‰ {len(sub_sub)} å€‹å­å­åˆ†é¡")
                                for ss_name, ss_url in sub_sub[:3]:  # åªæª¢æŸ¥å‰3å€‹
                                    html_ss = self.fetch_page(ss_url)
                                    if html_ss and self.is_endpoint_page(html_ss):
                                        print(
                                            f"      âœ… çµ‚é»é é¢: {ss_name} ({ss_url})"
                                        )
                                        self.endpoints.append(
                                            {
                                                "name": f"{name} - {sub_name} - {ss_name}",
                                                "url": ss_url,
                                                "type": "sub_sub",
                                                "parent": name,
                                                "sub_parent": sub_name,
                                            }
                                        )

            time.sleep(0.5)  # é¿å…éåº¦è«‹æ±‚

        self.categories = all_categories
        return all_categories

    def save_blueprint(self, filename="scraper_blueprint.json"):
        """å„²å­˜åˆ†é¡è—åœ–"""
        blueprint = {
            "categories": self.categories,
            "endpoints": self.endpoints,
            "metadata": {
                "total_categories": len(self.categories),
                "total_endpoints": len(self.endpoints),
                "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                "source": "https://tw.live",
            },
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(blueprint, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… åˆ†é¡è—åœ–å·²å„²å­˜: {filename}")
        print(f"ğŸ“‚ åˆ†é¡æ•¸: {len(self.categories)}")
        print(f"ğŸ¯ çµ‚é»é é¢æ•¸: {len(self.endpoints)}")

        return filename


def main():
    print("ğŸ” é–‹å§‹ç™¼ç¾ tw.live åˆ†é¡çµæ§‹...")
    print("=" * 60)

    discoverer = TWLiveCategoryDiscoverer()
    categories = discoverer.discover_categories()
    discoverer.save_blueprint()

    # é¡¯ç¤ºæ‘˜è¦
    print("\nğŸ“Š åˆ†é¡æ‘˜è¦:")
    print("=" * 60)
    for cat_name, subs in categories.items():
        print(f"  {cat_name}: {len(subs)} å€‹å­åˆ†é¡")

    print("\nğŸ¯ çµ‚é»é é¢æ‘˜è¦:")
    print("=" * 60)
    for endpoint in discoverer.endpoints[:10]:  # åªé¡¯ç¤ºå‰10å€‹
        print(f"  {endpoint['name']} ({endpoint['url']})")
    if len(discoverer.endpoints) > 10:
        print(f"  ... é‚„æœ‰ {len(discoverer.endpoints) - 10} å€‹çµ‚é»é é¢")


if __name__ == "__main__":
    main()
