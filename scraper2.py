#!/usr/bin/env python3
"""
tw.live ç›£æ§é»æŠ“å–å™¨ v2
åŸºæ–¼åˆ†é¡è—åœ–çš„å®Œæ•´æŠ“å–
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin


class TWLiveScraper2:
    def __init__(self):
        self.base_url = "https://tw.live"
        self.cameras = []
        self.seen_ids = set()
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )

    def fetch_page(self, url):
        """æŠ“å–é é¢ä¸¦è™•ç†éŒ¯èª¤"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
            return None

    def extract_camera_from_stack(self, stack, category_name):
        """å¾ cctv-stack æå–ç›£æ§é»è³‡è¨Š"""
        link = stack.find("a")
        if not link:
            return None

        camera_url = urljoin(self.base_url, link.get("href", ""))

        # æå– camera ID
        match = re.search(r"id=([a-zA-Z0-9_-]+)", camera_url)
        if not match:
            return None

        camera_id = match.group(1)

        # æå–åç¨±ï¼ˆå¾ç¸®åœ–é é¢ï¼‰
        name_tag = stack.find("p")
        name = name_tag.text.strip() if name_tag else "æœªçŸ¥ç›£æ§é»"

        # æå–ç¸®åœ– URL
        img = stack.find("img")
        thumbnail = img.get("data-src", "") if img else ""

        # å»ºç«‹åŸºæœ¬è³‡æ–™
        camera = {
            "id": camera_id,
            "name": name,
            "category": category_name,
            "location": category_name,  # é è¨­ä½¿ç”¨åˆ†é¡åç¨±ä½œç‚ºåœ°é»
            "url": camera_url,
            "thumbnail": thumbnail,
            "type": "image",  # é è¨­é¡å‹
            "imageUrl": thumbnail,  # é è¨­åœ–ç‰‡ç¶²å€
        }

        # åˆ¤æ–·é¡å‹ï¼ˆå¾ç¸®åœ– URLï¼‰
        if "youtube.com" in thumbnail:
            # YouTube é¡å‹
            yt_id_match = re.search(r"/vi/([a-zA-Z0-9_-]+)/", thumbnail)
            if yt_id_match:
                camera["type"] = "youtube"
                camera["youtubeId"] = yt_id_match.group(1)
                camera["description"] = f"{category_name} YouTube ç›´æ’­"
                # YouTube ä¸éœ€è¦ imageUrl
                del camera["imageUrl"]
        elif thumbnail.startswith("https://tw.live/assets/thumbnail.png"):
            # ä½”ä½ç¬¦åœ–ç‰‡ï¼Œå¯èƒ½éœ€è¦å¾è©³ç´°é é¢æå– HLS
            hls_url = self.extract_hls_from_detail_page(camera_url)
            if hls_url:
                camera["type"] = "hls"
                camera["hlsUrl"] = hls_url
                # HLS ä¸éœ€è¦ imageUrl
                del camera["imageUrl"]

        return camera

    def extract_hls_from_detail_page(self, detail_url):
        """å¾è©³ç´°é é¢æå– HLS URL"""
        html = self.fetch_page(detail_url)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # æŸ¥æ‰¾ HLS ä¾†æº
        source = soup.find("source", {"type": "application/x-mpegURL"})
        if source and source.get("src"):
            return source["src"]

        return None

    def scrape_endpoint_page(self, endpoint_info):
        """å¾çµ‚é»é é¢æŠ“å–æ‰€æœ‰ç›£æ§é»"""
        url = endpoint_info["url"]
        name = endpoint_info["name"]

        print(f"ğŸ“¹ æŠ“å–çµ‚é»: {name}")
        html = self.fetch_page(url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        cameras = []

        # æ‰¾åˆ°æ‰€æœ‰ç›£æ§é»é€£çµ
        cctv_stacks = soup.find_all("div", class_="cctv-stack")

        for stack in cctv_stacks:
            camera = self.extract_camera_from_stack(stack, name)
            if camera and camera["id"] not in self.seen_ids:
                cameras.append(camera)
                self.seen_ids.add(camera["id"])
                time.sleep(0.1)  # çŸ­å»¶é²

        print(f"  âœ… æ‰¾åˆ° {len(cameras)} å€‹ç›£æ§é»")
        return cameras

    def load_blueprint(self, filename="scraper_blueprint.json"):
        """è¼‰å…¥åˆ†é¡è—åœ–"""
        try:
            with open(filename, "r", encoding="utf-8") as f:
                blueprint = json.load(f)
            return blueprint
        except Exception as e:
            print(f"âŒ è¼‰å…¥è—åœ–å¤±æ•—: {e}")
            return None

    def scrape_from_blueprint(self):
        """å¾è—åœ–æŠ“å–æ‰€æœ‰ç›£æ§é»"""
        blueprint = self.load_blueprint()
        if not blueprint:
            return []

        endpoints = blueprint.get("endpoints", [])
        print(f"ğŸ¯ é–‹å§‹å¾ {len(endpoints)} å€‹çµ‚é»é é¢æŠ“å–ç›£æ§é»...")

        all_cameras = []

        # æ¸¬è©¦åªè™•ç†å‰ 5 å€‹çµ‚é»
        for endpoint in endpoints[:5]:
            cameras = self.scrape_endpoint_page(endpoint)
            all_cameras.extend(cameras)
            time.sleep(0.5)  # é¿å…éåº¦è«‹æ±‚

        self.cameras = all_cameras
        return all_cameras

    def save_test_output(self, filename="testoutput.json"):
        """å„²å­˜æ¸¬è©¦è¼¸å‡º"""
        output = {
            "cameras": self.cameras,
            "metadata": {
                "totalCount": len(self.cameras),
                "lastUpdated": time.strftime("%Y-%m-%d %H:%M:%S"),
                "version": "2.0.0",
                "source": "https://tw.live",
                "method": "blueprint-based",
            },
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… æ¸¬è©¦è¼¸å‡ºå·²å„²å­˜: {filename}")
        print(f"ğŸ“Š ç¸½å…± {len(self.cameras)} å€‹ç›£æ§é»")


def main():
    print("ğŸš€ é–‹å§‹æŠ“å– tw.live ç›£æ§é» (v2)...")
    print("=" * 60)

    scraper = TWLiveScraper2()
    cameras = scraper.scrape_from_blueprint()
    scraper.save_test_output()

    # é¡¯ç¤ºæ¨£æœ¬
    print("\nğŸ“‹ æ¨£æœ¬è³‡æ–™ (å‰ 5 å€‹):")
    print("=" * 60)
    for cam in cameras:
        print(json.dumps(cam, ensure_ascii=False, indent=2))
        print("-" * 40)


if __name__ == "__main__":
    main()
