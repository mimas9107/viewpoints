#!/usr/bin/env python3
"""
tw.live ç›£æ§é»çˆ¬èŸ²ç¨‹å¼
çˆ¬å–å…¨ç«™ç›£æ§é»è³‡è¨Šä¸¦å»ºç«‹è³‡æ–™åº«
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin
from datetime import datetime


class TWLiveScraper:
    def __init__(self):
        self.base_url = "https://tw.live"
        self.cameras = []
        self.categories = {}
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )

    def fetch_page(self, url):
        """æŠ“å–é é¢ä¸¦è™•ç†éŒ¯èª¤"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
            return None

    def extract_youtube_id(self, html):
        """å¾ HTML æå– YouTube ID"""
        patterns = [
            r"youtube\.com/embed/([a-zA-Z0-9_-]+)",
            r"youtube\.com/vi/([a-zA-Z0-9_-]+)",
            r"img\.youtube\.com/vi/([a-zA-Z0-9_-]+)",
            r'youtubeId[\'"]?\s*[:=]\s*[\'"]([a-zA-Z0-9_-]+)[\'"]',
        ]
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None

    def extract_hls_url(self, html):
        """å¾ HTML æå– HLS URL"""
        pattern = r'(https?://[^\s"\']+\.m3u8[^\s"\']*)'
        match = re.search(pattern, html)
        if match:
            return match.group(1)
        return None

    def extract_static_image_url(self, html):
        """å¾ HTML æå–éœæ…‹åœ–ç‰‡ URL"""
        patterns = [
            r'(https?://cctv[^\s"\']+\.(?:jpg|jpeg|png))',
            r'(https?://[^\s"\']+/abs2jpg\.php[^\s"\']*)',
        ]
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None

    def scrape_camera_detail(self, camera_url, camera_id):
        """çˆ¬å–å–®å€‹ç›£æ§é»è©³ç´°è³‡è¨Š"""
        print(f"  ğŸ“¹ æŠ“å–: {camera_id}")
        html = self.fetch_page(camera_url)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # æå–åŸºæœ¬è³‡è¨Š
        camera = {"id": camera_id, "url": camera_url}

        # æå–æ¨™é¡Œ
        h1 = soup.find("h1")
        if h1:
            camera["name"] = h1.text.strip().replace("å³æ™‚å½±åƒ", "").strip()

        # æå–æè¿°
        h2 = soup.find("h2")
        if h2:
            camera["description"] = h2.text.strip()

        # æå–å½±åƒä¾†æº
        figcaption = soup.find("figcaption")
        if figcaption:
            source_link = figcaption.find("a")
            if source_link:
                camera["source"] = source_link.text.strip()

        # åˆ¤æ–·ç›£æ§é¡å‹
        youtube_id = self.extract_youtube_id(html)
        if youtube_id:
            camera["type"] = "youtube"
            camera["youtubeId"] = youtube_id
        else:
            hls_url = self.extract_hls_url(html)
            if hls_url:
                camera["type"] = "hls"
                camera["hlsUrl"] = hls_url
            else:
                image_url = self.extract_static_image_url(html)
                if image_url:
                    camera["type"] = "image"
                    camera["imageUrl"] = image_url
                else:
                    camera["type"] = "unknown"

        return camera

    def scrape_category_page(self, category_url, category_name):
        """çˆ¬å–åˆ†é¡é é¢çš„æ‰€æœ‰ç›£æ§é»"""
        print(f"ğŸ—‚ï¸  æŠ“å–åˆ†é¡: {category_name}")
        html = self.fetch_page(category_url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        cameras = []

        # æ‰¾åˆ°æ‰€æœ‰ç›£æ§é»é€£çµ
        cctv_stacks = soup.find_all("div", class_="cctv-stack")

        for stack in cctv_stacks:
            link = stack.find("a")
            if not link:
                continue

            camera_url = urljoin(self.base_url, link.get("href", ""))

            # æå– camera ID
            match = re.search(r"id=([a-zA-Z0-9_-]+)", camera_url)
            if not match:
                continue

            camera_id = match.group(1)

            # æå–åç¨±ï¼ˆå¾ç¸®åœ–é é¢ï¼‰
            name_tag = stack.find("p")
            name = name_tag.text.strip() if name_tag else "æœªçŸ¥ç›£æ§é»"

            # æå–ç¸®åœ– URL
            img = stack.find("img")
            thumbnail = img.get("data-src", "") if img else ""

            # å»ºç«‹åŸºæœ¬è³‡æ–™
            camera = {
                "id": camera_id,
                "name": name,
                "category": category_name,
                "url": camera_url,
                "thumbnail": thumbnail,
            }

            # åˆ¤æ–·é¡å‹ï¼ˆå¾ç¸®åœ– URLï¼‰
            if "youtube.com" in thumbnail:
                # YouTube é¡å‹
                yt_id_match = re.search(r"/vi/([a-zA-Z0-9_-]+)/", thumbnail)
                if yt_id_match:
                    camera["type"] = "youtube"
                    camera["youtubeId"] = yt_id_match.group(1)
                    camera["description"] = f"{category_name} YouTube ç›´æ’­"

            cameras.append(camera)

            # é¿å…éåº¦è«‹æ±‚
            time.sleep(0.5)

        print(f"  âœ… æ‰¾åˆ° {len(cameras)} å€‹ç›£æ§é»")
        return cameras

    def scrape_all_categories(self):
        """çˆ¬å–æ‰€æœ‰ä¸»è¦åˆ†é¡"""
        categories = {
            "åœ‹é“": [
                ("/national-highway/1/", "åœ‹é“ä¸€è™Ÿ"),
                ("/national-highway/3/", "åœ‹é“ä¸‰è™Ÿ"),
                ("/national-highway/5/", "åœ‹é“äº”è™Ÿ"),
            ],
            "æ™¯é»": [
                ("/yms/", "é™½æ˜å±±"),
                ("/hhs/", "åˆæ­¡å±±"),
                ("/np/sheipa/", "é›ªéœ¸åœ‹å®¶å…¬åœ’"),
                ("/np/alishan/", "é˜¿é‡Œå±±"),
                ("/sunmoonlake/", "æ—¥æœˆæ½­"),
            ],
            "å¸‚å€": [
                ("/city/taipeicity/", "å°åŒ—å¸‚"),
                ("/city/newtaipeicity/", "æ–°åŒ—å¸‚"),
                ("/city/taichungcity/", "å°ä¸­å¸‚"),
                ("/city/tainancity/", "å°å—å¸‚"),
                ("/city/kaohsiungcity/", "é«˜é›„å¸‚"),
            ],
            "çœé“": [
                ("/beiyi/", "åŒ—å®œå…¬è·¯"),
                ("/provincial-highway/suhua/", "è˜‡èŠ±æ”¹"),
                ("/provincial-highway/newtaipei/", "æ–°åŒ—å¸‚çœé“"),
            ],
            "å¿«é€Ÿé“è·¯": [
                ("/provincial-highway/64/", "å°64ç·š"),
                ("/provincial-highway/61/", "å°61ç·š"),
                ("/provincial-highway/66/", "å°66ç·š"),
            ],
        }

        all_cameras = []

        for cat_name, urls in categories.items():
            self.categories[cat_name] = []

            for url, sub_name in urls:
                full_url = urljoin(self.base_url, url)
                cameras = self.scrape_category_page(full_url, sub_name)

                all_cameras.extend(cameras)
                self.categories[cat_name].extend(cameras)

                # é¿å…éåº¦è«‹æ±‚
                time.sleep(1)

        self.cameras = all_cameras
        return all_cameras

    def save_database(self, filename="cameras_database.json"):
        """å„²å­˜è³‡æ–™åº«"""
        database = {
            "cameras": self.cameras,
            "categories": self.categories,
            "metadata": {
                "totalCount": len(self.cameras),
                "lastUpdated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "version": "1.0.0",
                "source": "https://tw.live",
            },
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(database, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… è³‡æ–™åº«å·²å„²å­˜: {filename}")
        print(f"ğŸ“Š ç¸½å…± {len(self.cameras)} å€‹ç›£æ§é»")
        print(f"ğŸ“ åˆ†é¡æ•¸: {len(self.categories)}")

        return filename


def main():
    print("ğŸš€ é–‹å§‹çˆ¬å– tw.live ç›£æ§é»è³‡æ–™...")
    print("=" * 60)

    scraper = TWLiveScraper()

    # çˆ¬å–æ‰€æœ‰åˆ†é¡
    cameras = scraper.scrape_all_categories()

    # å„²å­˜è³‡æ–™åº«
    scraper.save_database()

    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    print("\nğŸ“ˆ çµ±è¨ˆè³‡è¨Š:")
    print("=" * 60)
    for cat_name, cams in scraper.categories.items():
        print(f"  {cat_name}: {len(cams)} å€‹")

    # é¡¯ç¤ºæ¨£æœ¬
    print("\nğŸ“‹ æ¨£æœ¬è³‡æ–™ (å‰ 3 å€‹):")
    print("=" * 60)
    for cam in cameras[:3]:
        print(json.dumps(cam, ensure_ascii=False, indent=2))
        print("-" * 40)


if __name__ == "__main__":
    main()
