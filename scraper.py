#!/usr/bin/env python3
"""
tw.live ç›£æ§é»çˆ¬èŸ²ç¨‹å¼
çˆ¬å–å…¨ç«™ç›£æ§é»è³‡è¨Šä¸¦å»ºç«‹è³‡æ–™åº«
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from urllib.parse import urljoin
from datetime import datetime


class TWLiveScraper:
    def __init__(self):
        self.base_url = "https://tw.live"
        self.cameras = []
        self.categories = {}
        self.seen_ids = set()
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )

    def fetch_page(self, url):
        """æŠ“å–é é¢ä¸¦è™•ç†éŒ¯èª¤"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•— {url}: {e}")
            return None

    def extract_youtube_id(self, html):
        """å¾ HTML æå– YouTube ID"""
        patterns = [
            r"youtube\.com/embed/([a-zA-Z0-9_-]+)",
            r"youtube\.com/vi/([a-zA-Z0-9_-]+)",
            r"img\.youtube\.com/vi/([a-zA-Z0-9_-]+)",
            r'youtubeId[\'"]?\s*[:=]\s*[\'"]([a-zA-Z0-9_-]+)[\'"]',
        ]
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None

    def extract_hls_url(self, html):
        """å¾ HTML æå– HLS URL"""
        pattern = r'(https?://[^\s"\']+\.m3u8[^\s"\']*)'
        match = re.search(pattern, html)
        if match:
            return match.group(1)
        return None

    def extract_static_image_url(self, html):
        """å¾ HTML æå–éœæ…‹åœ–ç‰‡ URL"""
        patterns = [
            r'(https?://cctv[^\s"\']+\.(?:jpg|jpeg|png))',
            r'(https?://[^\s"\']+/abs2jpg\.php[^\s"\']*)',
        ]
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None

    def scrape_camera_detail(self, camera_url, camera_id):
        """çˆ¬å–å–®å€‹ç›£æ§é»è©³ç´°è³‡è¨Š"""
        print(f"  ğŸ“¹ æŠ“å–: {camera_id}")
        html = self.fetch_page(camera_url)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # æå–åŸºæœ¬è³‡è¨Š
        camera = {"id": camera_id, "url": camera_url}

        # æå–æ¨™é¡Œ
        h1 = soup.find("h1")
        if h1:
            camera["name"] = h1.text.strip().replace("å³æ™‚å½±åƒ", "").strip()

        # æå–æè¿°
        h2 = soup.find("h2")
        if h2:
            camera["description"] = h2.text.strip()

        # æå–å½±åƒä¾†æº
        figcaption = soup.find("figcaption")
        if figcaption:
            source_link = figcaption.find("a")
            if source_link:
                camera["source"] = source_link.text.strip()

        # åˆ¤æ–·ç›£æ§é¡å‹
        youtube_id = self.extract_youtube_id(html)
        if youtube_id:
            camera["type"] = "youtube"
            camera["youtubeId"] = youtube_id
        else:
            hls_url = self.extract_hls_url(html)
            if hls_url:
                camera["type"] = "hls"
                camera["hlsUrl"] = hls_url
            else:
                image_url = self.extract_static_image_url(html)
                if image_url:
                    camera["type"] = "image"
                    camera["imageUrl"] = image_url
                else:
                    camera["type"] = "unknown"

        return camera

    def scrape_category_page(self, category_url, category_name):
        """çˆ¬å–åˆ†é¡é é¢çš„æ‰€æœ‰ç›£æ§é»"""
        print(f"ğŸ—‚ï¸  æŠ“å–åˆ†é¡: {category_name}")
        html = self.fetch_page(category_url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        cameras = []

        # æ‰¾åˆ°æ‰€æœ‰ç›£æ§é»é€£çµ
        cctv_stacks = soup.find_all("div", class_="cctv-stack")

        if cctv_stacks:
            # ç›´æ¥æœ‰ cctv-stack çš„é é¢
            for stack in cctv_stacks:
                camera = self.extract_camera_from_stack(stack, category_name)
                if camera and camera["id"] not in self.seen_ids:
                    cameras.append(camera)
                    self.seen_ids.add(camera["id"])
                    time.sleep(0.5)
        else:
            # ç¸½è¦½é é¢ï¼Œéœ€è¦è§£æå­é é¢é€£çµ
            sub_links = self.extract_sub_page_links(soup, category_url)
            for sub_url in sub_links:
                sub_cameras = self.scrape_category_page(sub_url, category_name)
                cameras.extend(sub_cameras)

        print(f"  âœ… æ‰¾åˆ° {len(cameras)} å€‹ç›£æ§é»")
        return cameras

    def extract_camera_from_stack(self, stack, category_name):
        """å¾ cctv-stack æå–ç›£æ§é»è³‡è¨Š"""
        link = stack.find("a")
        if not link:
            return None

        camera_url = urljoin(self.base_url, link.get("href", ""))

        # æå– camera ID
        match = re.search(r"id=([a-zA-Z0-9_-]+)", camera_url)
        if not match:
            return None

        camera_id = match.group(1)

        # æå–åç¨±ï¼ˆå¾ç¸®åœ–é é¢ï¼‰
        name_tag = stack.find("p")
        name = name_tag.text.strip() if name_tag else "æœªçŸ¥ç›£æ§é»"

        # æå–ç¸®åœ– URL
        img = stack.find("img")
        thumbnail = img.get("data-src", "") if img else ""

        # å»ºç«‹åŸºæœ¬è³‡æ–™
        camera = {
            "id": camera_id,
            "name": name,
            "category": category_name,
            "location": category_name,  # é è¨­ä½¿ç”¨åˆ†é¡åç¨±ä½œç‚ºåœ°é»
            "url": camera_url,
            "thumbnail": thumbnail,
            "type": "image",  # é è¨­é¡å‹
            "imageUrl": thumbnail,  # é è¨­åœ–ç‰‡ç¶²å€
        }

        # åˆ¤æ–·é¡å‹ï¼ˆå¾ç¸®åœ– URLï¼‰
        if "youtube.com" in thumbnail:
            # YouTube é¡å‹
            yt_id_match = re.search(r"/vi/([a-zA-Z0-9_-]+)/", thumbnail)
            if yt_id_match:
                camera["type"] = "youtube"
                camera["youtubeId"] = yt_id_match.group(1)
                camera["description"] = f"{category_name} YouTube ç›´æ’­"
                # YouTube ä¸éœ€è¦ imageUrl
                del camera["imageUrl"]

        return camera

    def extract_sub_page_links(self, soup, base_url):
        """å¾ç¸½è¦½é é¢æå–å­é é¢é€£çµ"""
        sub_links = []
        # æŸ¥æ‰¾åŒ…å« "å³æ™‚å½±åƒ" çš„æŒ‰éˆ•é€£çµ
        buttons = soup.find_all("a", class_="btn")
        for btn in buttons:
            if btn.get_text(strip=True) == "å³æ™‚å½±åƒ":
                href = btn.get("href")
                if href:
                    full_url = urljoin(self.base_url, href)
                    sub_links.append(full_url)

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼ŒæŸ¥æ‰¾ cctv-menu ä¸­çš„æ‰€æœ‰é€£çµï¼ˆç”¨æ–¼å¸‚å€åˆ†å€ï¼‰
        if not sub_links:
            cctv_menu = soup.find("div", class_="cctv-menu")
            if cctv_menu:
                links = cctv_menu.find_all("a")
                for link in links:
                    href = link.get("href")
                    if href and not href.startswith("#"):
                        full_url = urljoin(self.base_url, href)
                        sub_links.append(full_url)

        return sub_links

    def scrape_all_categories(self):
        """çˆ¬å–æ‰€æœ‰ä¸»è¦åˆ†é¡"""
        categories = {
            "åœ‹é“": [
                ("/national-highway/1/", "åœ‹é“ä¸€è™Ÿ"),
                ("/national-highway/2/", "åœ‹é“äºŒè™Ÿ"),
                ("/national-highway/3/", "åœ‹é“ä¸‰è™Ÿ"),
                ("/national-highway/3a/", "åœ‹ä¸‰ç”²ç·š"),
                ("/national-highway/4/", "åœ‹é“å››è™Ÿ"),
                ("/national-highway/5/", "åœ‹é“äº”è™Ÿ"),
                ("/national-highway/6/", "åœ‹é“å…­è™Ÿ"),
                ("/national-highway/8/", "åœ‹é“å…«è™Ÿ"),
                ("/national-highway/10/", "åœ‹é“åè™Ÿ"),
                ("/national-highway/khport-viaduct/", "é«˜æ¸¯é«˜æ¶"),
            ],
            "å¿«é€Ÿé“è·¯": [
                ("/provincial-highway/61/", "å°61ç·š"),
                ("/provincial-highway/62/", "å°62ç·š"),
                ("/provincial-highway/64/", "å°64ç·š"),
                ("/provincial-highway/66/", "å°66ç·š"),
                ("/provincial-highway/68/", "å°68ç·š"),
                ("/provincial-highway/72/", "å°72ç·š"),
                ("/provincial-highway/74/", "å°74ç·š"),
                ("/provincial-highway/76/", "å°76ç·š"),
                ("/provincial-highway/78/", "å°78ç·š"),
                ("/provincial-highway/82/", "å°82ç·š"),
                ("/provincial-highway/84/", "å°84ç·š"),
                ("/provincial-highway/86/", "å°86ç·š"),
                ("/provincial-highway/88/", "å°88ç·š"),
            ],
            "çœé“": [
                ("/beiyi/", "åŒ—å®œå…¬è·¯"),
                ("/provincial-highway/suhua/", "è˜‡èŠ±æ”¹"),
                ("/scih/", "å—æ©«å…¬è·¯"),
                ("/provincial-highway/keelung/", "åŸºéš†å¸‚"),
                ("/provincial-highway/newtaipei/", "æ–°åŒ—å¸‚"),
                ("/provincial-highway/taoyuan/", "æ¡ƒåœ’å¸‚"),
                ("/provincial-highway/hsinchucity/", "æ–°ç«¹å¸‚"),
                ("/provincial-highway/hsinchucounty/", "æ–°ç«¹ç¸£"),
                ("/provincial-highway/miaoli/", "è‹—æ —ç¸£"),
                ("/provincial-highway/taichung/", "å°ä¸­å¸‚"),
                ("/provincial-highway/changhua/", "å½°åŒ–ç¸£"),
                ("/provincial-highway/nantou/", "å—æŠ•ç¸£"),
                ("/provincial-highway/yunlin/", "é›²æ—ç¸£"),
                ("/provincial-highway/chiayicity/", "å˜‰ç¾©å¸‚"),
                ("/provincial-highway/chiayicounty/", "å˜‰ç¾©ç¸£"),
                ("/provincial-highway/tainan/", "å°å—å¸‚"),
                ("/provincial-highway/kaohsiung/", "é«˜é›„å¸‚"),
                ("/provincial-highway/pingtung/", "å±æ±ç¸£"),
                ("/provincial-highway/taitung/", "å°æ±ç¸£"),
                ("/provincial-highway/hualien/", "èŠ±è“®ç¸£"),
                ("/provincial-highway/yilan/", "å®œè˜­ç¸£"),
            ],
            "å¸‚å€": [
                ("/city/taipeicity/", "å°åŒ—å¸‚"),
                ("/city/newtaipeicity/", "æ–°åŒ—å¸‚"),
                ("/city/taoyuancity/", "æ¡ƒåœ’å¸‚"),
                ("/city/taichungcity/", "å°ä¸­å¸‚"),
                ("/city/tainancity/", "å°å—å¸‚"),
                ("/city/kaohsiungcity/", "é«˜é›„å¸‚"),
                ("/county/keelungcity/", "åŸºéš†å¸‚"),
                ("/county/hsinchucity/", "æ–°ç«¹å¸‚"),
                ("/county/hsinchucounty/", "æ–°ç«¹ç¸£"),
                ("/county/changhuacounty/", "å½°åŒ–ç¸£"),
                ("/county/yunlincounty/", "é›²æ—ç¸£"),
                ("/county/chiayicity/", "å˜‰ç¾©å¸‚"),
                ("/county/chiayicounty/", "å˜‰ç¾©ç¸£"),
                ("/county/pingtungcounty/", "å±æ±ç¸£"),
                ("/county/taitungcounty/", "å°æ±ç¸£"),
                ("/county/yilancounty/", "å®œè˜­ç¸£"),
                ("/county/penghucounty/", "æ¾æ¹–ç¸£"),
                ("/county/lienchiangcounty/", "é€£æ±Ÿç¸£"),
            ],
            "åœ‹å®¶å…¬åœ’": [
                ("/yms/", "é™½æ˜å±±"),
                ("/np/sheipa/", "é›ªéœ¸"),
                ("/hhs/", "å¤ªé­¯é–£(åˆæ­¡å±±)"),
                ("/np/yushan/", "ç‰å±±"),
                ("/np/taijiang/", "å°æ±Ÿ"),
                ("/np/kenting/", "å¢¾ä¸"),
                ("/np/kinmen/", "é‡‘é–€"),
            ],
            "é¢¨æ™¯å€": [
                ("/nsa/necoast/", "æ±åŒ—è§’"),
                ("/nsa/northguan/", "åŒ—æµ·å²¸"),
                ("/nsa/trimt/", "åƒå±±"),
                ("/np/wuling/", "æ­¦é™µè¾²å ´"),
                ("/fss/", "ç¦å£½å±±è¾²å ´"),
                ("/sunmoonlake/", "æ—¥æœˆæ½­"),
                ("/np/alishan/", "é˜¿é‡Œå±±"),
                ("/nsa/swcoast/", "é›²å˜‰å—æ¿±æµ·"),
                ("/nsa/siraya/", "è¥¿æ‹‰é›…"),
                ("/nsa/maolin/", "èŒ‚æ—"),
                ("/eastcoast/", "æ±éƒ¨æµ·å²¸"),
                ("/erv/", "èŠ±æ±ç¸±è°·"),
                ("/nsa/penghu/", "æ¾æ¹–"),
            ],
            "å…¶ä»–": [
                ("/cwb/", "ä¸­å¤®æ°£è±¡ç½²"),
                ("/namr/", "åœ‹å®¶æµ·æ´‹ç ”ç©¶é™¢"),
                ("/epa/", "ç’°ä¿ç½²"),
                ("/wra/", "æ°´åˆ©ç½²"),
                ("/sp/", "ç§‘å­¸åœ’å€"),
                ("/wra/tsengwen/", "æ›¾æ–‡æ°´åº«"),
            ],
        }

        all_cameras = []

        for cat_name, urls in categories.items():
            self.categories[cat_name] = []

            for url, sub_name in urls:
                full_url = urljoin(self.base_url, url)
                cameras = self.scrape_category_page(full_url, sub_name)

                all_cameras.extend(cameras)
                self.categories[cat_name].extend(cameras)

                # é¿å…éåº¦è«‹æ±‚
                time.sleep(1)

        self.cameras = all_cameras
        return all_cameras

    def save_database(self, filename="cameras_database.json"):
        """å„²å­˜è³‡æ–™åº«"""
        database = {
            "cameras": self.cameras,
            "categories": self.categories,
            "metadata": {
                "totalCount": len(self.cameras),
                "lastUpdated": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "version": "1.0.0",
                "source": "https://tw.live",
            },
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(database, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… è³‡æ–™åº«å·²å„²å­˜: {filename}")
        print(f"ğŸ“Š ç¸½å…± {len(self.cameras)} å€‹ç›£æ§é»")
        print(f"ğŸ“ åˆ†é¡æ•¸: {len(self.categories)}")

        return filename


def main():
    print("ğŸš€ é–‹å§‹çˆ¬å– tw.live ç›£æ§é»è³‡æ–™...")
    print("=" * 60)

    scraper = TWLiveScraper()

    # çˆ¬å–æ‰€æœ‰åˆ†é¡
    cameras = scraper.scrape_all_categories()

    # å„²å­˜è³‡æ–™åº«
    scraper.save_database()

    # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
    print("\nğŸ“ˆ çµ±è¨ˆè³‡è¨Š:")
    print("=" * 60)
    for cat_name, cams in scraper.categories.items():
        print(f"  {cat_name}: {len(cams)} å€‹")

    # é¡¯ç¤ºæ¨£æœ¬
    print("\nğŸ“‹ æ¨£æœ¬è³‡æ–™ (å‰ 3 å€‹):")
    print("=" * 60)
    for cam in cameras[:3]:
        print(json.dumps(cam, ensure_ascii=False, indent=2))
        print("-" * 40)


if __name__ == "__main__":
    main()
